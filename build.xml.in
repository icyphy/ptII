<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- $Id$
     $PTII/configure reads in build.xml.in and creates build.xml.
     If configure is not available, copy build.xml.default to build.xml. -->
<project basedir="." default="build" name="ptII" 
	 @ANT_MAVEN_ARTIFACT@
	 >
  <!-- We can refer to any environment variable FOO as ${env.FOO} -->
  <property environment="env" />


  <!-- Ant properties. Alphabetize, please -->

  <!-- checkstyle.formatter defaults to xml and is used by checkstyle and test.  Try also -Dcheckstyle.formater=html. -->
  <property name="checkstyle.formatter" value="xml" />

  <!-- The location of the Cobertura directory, used for code coverage -->
  <property name="cobertura.dir" value="@COBERTURA_DIR@" />

  <property name="debuglevel" value="source,lines,vars" />

  <!-- Location of spotbugs, which does static analysis on the code.
    -->
  <property name="spotbugs.home" value="${basedir}/vendors/spotbugs-3.1.1" />

  <!-- The value of the memorymaximumsize javac and maxmemorysize and java parameter 
       We use 1400m so that we don't have problems under 32-bit
       Windows.  See
       http://javarevisited.blogspot.jp/2013/04/what-is-maximum-heap-size-for-32-bit-64-JVM-Java-memory.html-->
  <property name="java.maxmemory" value="1400m" />

  <!-- The value of the maxmemorysize and java parameter for tests
       that consume a lot of memory.  This will probably not work
       under 32-bit Windows.  See
       http://javarevisited.blogspot.jp/2013/04/what-is-maximum-heap-size-for-32-bit-64-JVM-Java-memory.html

       Too large a value here, such as 3500 will cause Travis to fail
       with a message about being unable to fork.
  -->
  <property name="java.maxmemory.large" value="3200m" />

  <!-- Packages to exclude from targets that call javadoc.
       Unfortunately, <fileset> results in a command line that is too long.
       Grab the packages from ptII.excludes, put them in a file called /tmp/e and run
       awk -F '"' '{print $2}' /tmp/e | sed 's@/$@@' | sed 's@/@.@g' | sort | awk '{printf("%s.*,", $1)}'
       (Lame) -->
  <!-- Javadoc 10 can't handle UDPSocketHelper:
           [javadoc] javadoc: error - An internal exception has occurred. 
           [javadoc] 	(com.sun.tools.javac.code.ClassFinder$BadClassFile: bad class file: /Users/cxh/src/ptII11.0.devel/ptolemy/actor/lib/jjs/modules/udpSocket/UDPSocketHelper$UDPSocket$1$2.class       
  -->
  <property name="javadoc.excludepackagenames" value="@ANT_JAVADOC_EXCLUDEPACKAGENAMES@,org.terraswarm.accessor.accessors.web.node_modules.*,org.terraswarm.accessor.accessors-svn,ptolemy.apps.*,ptolemy.actor.ptalon.demo.ptinyos.SendAndReceiveCnt.output,ptolemy.actor.ptalon.demo.ptinyos.SenseToLeds.output,ptolemy.domains.ptinyos.demo.CntToLedsAndRfm.output,ptolemy.domains.ptinyos.demo.RfmToLeds.output,ptolemy.domains.ptinyos.demo.SendAndReceiveCnt.output,ptolemy.domains.ptinyos.demo.SenseToLeds.output,ptolemy.actor.lib.jjs.modules.udpSocket.*" />


  <!-- Packages on which doccheck, javadoc and ojdcheck are run. -->
  <property name="javadoc.packagenames" value="com.*,diva.*,lbnl.*,org.hlacerti.*,org.json.*,org.ptolemy.*,org.terraswarm.*,ptolemy.*,ptserver.*" />

  <!-- The location of the jsdoc distribution, used by the jsdoc target. -->
  <property name="jsdoc.home" value="node_modules/@terraswarm/jsdoc" />

  <!-- The location of the jsdoc command, used by the jsdoc target. -->
  <property name="jsdoc.command" value="${jsdoc.home}/jsdoc.js" />
    
  <!-- Location of the jshint executable, used to check for common errors in JavaScript files. -->
  <property name="jshint.executable" value="${env.HOME}/node_modules/jshint/bin/jshint"/>

  <!-- junit.formatter defaults to xml and is used by checkstyle and test.  Other values: brief, failure, plain -->
  <property name="junit.formatter" value="xml" />

  <!-- junit.single.formatter defaults to plain and is used by test.single.  Other values: brief, failure, plain -->
  <property name="junit.single.formatter" value="plain" />

  <!-- ptII.reports contains reports generated by checkstyle, spotbugs and test.
       ptII.reports must be declared before any properties that refer to it. (Lame).
    -->
  <property name="ptII.reports" value="${basedir}/reports" />

  <!-- The output director for junit. -->
  <property name="junit.output.dir" value="${ptII.reports}/junit" />

  <!-- The location of the file that contains the certificates used to sign JNLP files.
       See $PTII/mk/jnlp.mk for details -->
  <property name="keystore" value="ptKeystore"/>

  <!-- model names the model to be opened.  The default is the empty string.  To use this, run 'ant vergil -Dmodel=foo.xml' -->
  <property name="model" value="" />

  <property name="source" value="@JVERSION_TARGET@" />

  <property name="target" value="@JVERSION_TARGET@" />

  <!-- The pathname of the tests that are run by the test.cobertura, test.report, test.short targets. -->
  <property name="test.batch" value="**/junit/*.java" />

  <!-- The classname of the single JUnit test that is run by the test.single target. -->
  <property name="test.name" value="ptolemy.actor.lib.test.junit.JUnitTclTest" />

  <!-- The timeout in seconds for spotbugs.
       See also the timeout= values in ptolemy/util/test/junit/*.java. -->
  <property name="timeout" value="2400000" />

  <!-- The timeout in seconds for short running tests.
       See also the timeout= values in ptolemy/util/test/junit/*.java.
       Originally, 2400000.
       10000 was too short for ptolemy.cg.kernel.generic.program.procedural.java.modular.test.junit -->
  <property name="timeout.short" value="1000000" />

  <!-- The timeout in seconds for long running junit tests.
       See also the timeout= values in ptolemy/util/test/junit/*.java. -->
  <property name="timeout.long" value="4000000" />

  <!-- The timeout in seconds for longest running junit tests.
       See also the timeout= values in ptolemy/util/test/junit/*.java. -->
  <property name="timeout.longest" value="19000000" />

  <!-- The timeout in milliseconds for the Node Package Manager (npm) -->
  <property name="timeout.npm" value="45000" />

  <!-- Ant paths to optional jar files set by the configure script. -->
  <path id="ptII.classpath.optional">
    @ANT_CLASSPATH@
  </path>

  <!-- Whether output is sent to a file.  Used by test.single -->
  <property name="usefile" value="false"/>
  
  <!-- The optional jar files as a property, used in subant. -->
  <pathconvert property="ptII.classpath.optional.property" refid="ptII.classpath.optional"/>

  <!-- Ant paths. Alphabetize, please -->

  <!-- Path for Cobertura, used by code coverage. -->
  <path id="cobertura.classpath">
    <fileset dir="${cobertura.dir}">
      <include name="@COBERTURA_JAR@" />
      <include name="lib/*.jar"/>
    </fileset>
  </path>

  <!-- The Ptolemy II Classpath -->
  <path id="ptII.classpath">
    <pathelement location="${env.HOME}/.ptolemyII" />
    <!-- Java 1.6 and later: * in the classpath matches *.jar files -->
    <pathelement location="${env.HOME}/.ptolemyII/*" />
    <pathelement location="${basedir}" />
    <path refid="ptII.classpath.optional"/>
    <!-- Include ptolemy/moml/test so that ptolemy/moml/test/MoMLParserNoPackage.tcl passes -->
    <pathelement location="ptolemy/moml/test"/>
  </path>

  <!-- Ant patternsets. Alphabetize, please -->
  <patternset id="ptII.excludes">
    @ANT_ALWAYS_EXCLUDE@
    @ANT_EXCLUDE@
  </patternset>  

  <!-- There are several categories of test, 32-bit tests, regular tests, long tests
       and longest tests.  We have excludes for each category. -->

  <!-- Tests that we want to exclude because they don't work or are called by AllTests. -->
  <patternset id="test.32bit.excludes">
    <exclude name="**/junit/JUnitAuto32Test.*" />
  </patternset>

  <!-- First batch of tests for Cape Code.  The CapeCode tests are
       split in to two batches so that Travis can build them without
       timing out.  See
       $PTII/org/terraswarm/accessor/accessors/web/build.xml for
       non-CapeCode tests.-->
  <patternset id="test.capecode1.includes">
    <include name="**/ptolemy/actor/lib/jjs/modules/a*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/b*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/c*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/d*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/e*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/f*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/e*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/g*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/h*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/i*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/j*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/k*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/m*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/n*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/o*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/p*/**/junit/JUnitTclTest.java" />
  </patternset>
  
  <!-- Exclude Cape Code tests from short, long and longest. -->
  <patternset id="test.capecode1.excludes">
    <exclude name="**/ptolemy/actor/lib/jjs/modules/a*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/b*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/c*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/d*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/e*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/f*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/e*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/g*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/h*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/i*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/j*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/k*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/m*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/n*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/o*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/p*/**/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Second batch of tests for Cape Code. --> 
  <patternset id="test.capecode2.includes">
    <include name="**/ptolemy/actor/lib/jjs/modules/q*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/r*/**/junit/JUnitTclTest.java" />
    <!-- socket and serial take a long time. -->
    <include name="**/ptolemy/actor/lib/jjs/modules/s*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/t*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/u*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/v*/**/junit/JUnitTclTest.java" />        
    <!-- websocket gets run in capecode3 -->
    <include name="**/ptolemy/actor/lib/jjs/modules/x*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/y*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/modules/z*/**/junit/JUnitTclTest.java" />
  </patternset>

  <!-- Exclude Cape Code tests from short, long and longest. -->
  <patternset id="test.capecode2.excludes">
    <!-- Note that certain jjs/modules/tests are excluded from Travis, see ptolemy/util/test/junit/AutoTests.java -->
    <exclude name="**/ptolemy/actor/lib/jjs/modules/q*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/r*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/s*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/t*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/u*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/v*/**/junit/JUnitTclTest.*" />        

    <exclude name="**/ptolemy/actor/lib/jjs/modules/x*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/y*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/modules/z*/**/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Second batch of tests for Cape Code. --> 
  <patternset id="test.capecode3.includes">
    <!-- websocket takes a long time, so we run it here. -->
    <!-- Note that certain jjs/modules/tests are excluded from Travis, see ptolemy/util/test/junit/AutoTests.java -->
    <include name="**/ptolemy/actor/lib/jjs/modules/w*/**/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/actor/lib/jjs/test/junit/JUnitTclTest.java" />
    <include name="**/org/terraswarm/accessor/test/junit/JUnitTclTest.java" />
    <include name="**/org/terraswarm/accessor/accessors/web/**/junit/JUnitTclTest.java" />
  </patternset>

  <!-- Exclude Cape Code tests from short, long and longest. -->
  <patternset id="test.capecode3.excludes">
    <exclude name="**/ptolemy/actor/lib/jjs/modules/w*/**/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/actor/lib/jjs/test/junit/JUnitTclTest.*" />
    <exclude name="**/org/terraswarm/accessor/test/junit/JUnitTclTest.*" />
    <exclude name="**/org/terraswarm/accessor/accessors/web/**/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Tests of the core, first batch. -->
  <patternset id="test.core1.includes">
    <include name="**/ptolemy/actor/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/data/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/graph/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/gui/**/test/junit/JUnitTclTest.java" />
  </patternset>

  <patternset id="test.core1.excludes">
    <exclude name="**/ptolemy/actor/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/data/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/graph/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/gui/**/test/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Tests of the core, first batch. -->
  <patternset id="test.core2.includes">
    <include name="**/ptolemy/domains/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/kernel/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/math/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/media/**/test/junit/JUnitTclTest.java" />
  </patternset>

  <patternset id="test.core2.excludes">
    <exclude name="**/ptolemy/domains/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/kernel/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/math/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/media/**/test/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Tests of the core, third batch. -->
  <patternset id="test.core3.includes">
    <include name="**/ptolemy/cg/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/demo/**/test/junit/JUnitTclTest.java" />
    <include name="**/adm/test/junit/JUnitTclTest.java" />
  </patternset>

  <patternset id="test.core3.excludes">
    <exclude name="**/ptolemy/cg/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/demo/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/adm/test/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Tests of the core, fourth batch. -->
  <patternset id="test.core4.includes">
    <!-- Note that hlacerti tests are excluded from Travis, see ptolemy/util/test/junit/AutoTests.java -->
    <include name="**/org/hlacerti/**/test/junit/JUnitTclTest.java" />
    <include name="**/org/ptolemy/**/test/junit/JUnitTclTest.java" />
    <include name="**/org/terraswarm/ros/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/caltrop/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/distributed/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/moml/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/homer/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/util/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/vergil/**/test/junit/JUnitTclTest.java" />
    <include name="**/ptserver/**/test/junit/JUnitTclTest.java" />
  </patternset>

  <patternset id="test.core4.excludes">
    <exclude name="**/org/hlacerti/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/org/ptolemy/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/org/terraswarm/ros/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/caltrop/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/distributed/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/homer/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/moml/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/util/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/vergil/**/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptserver/**/test/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Tests of the core, fifth batch. -->
  <patternset id="test.core5.includes">
    <include name="**/ptolemy/configs/test/junit/JUnitTclTest.java" />
  </patternset>

  <patternset id="test.core5.excludes">
    <exclude name="**/ptolemy/configs/test/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Tests of the core, sixth batch. -->
  <patternset id="test.core6.includes">
    <include name="**/ptolemy/configs/test/constraints/test/junit/JUnitTclTest.java" />
  </patternset>

  <patternset id="test.core6.excludes">
    <exclude name="**/ptolemy/configs/test/constraints/test/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Tests of the core, seventh batch. -->
  <patternset id="test.core7.includes">
    <include name="**/ptolemy/domains/csp/test/junit/JUnitTclTest.java" />
    <include name="**/doc/**/test/junit/JUnitTclTest.java" />
  </patternset>

  <patternset id="test.core7.excludes">
    <exclude name="**/ptolemy/domains/csp/test/junit/JUnitTclTest.*" />
    <exclude name="**/doc/**/test/junit/JUnitTclTest.*" />
  </patternset>

  <!-- Tests of the export demos -->
  <patternset id="test.export1.includes">
    <include name="ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTestBatch1.java" />
  </patternset>

  <patternset id="test.export2.includes">
    <include name="ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTestBatch2.java" />
  </patternset>

  <patternset id="test.export3.includes">
    <include name="ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTestBatch3.java" />
  </patternset>

  <patternset id="test.export4.includes">
    <include name="ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTestBatch4.java" />
  </patternset>

  <patternset id="test.export5.includes">
    <include name="ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTestBatch5.java" />
  </patternset>

  <patternset id="test.export6.includes">
    <include name="ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTestBatch6.java" />
  </patternset>

  <patternset id="test.export7.includes">
    <include name="ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTestBatch7.java" />
  </patternset>
  
  <!-- Tests that we want to exclude because they don't work or are called by AllTests. -->
  <patternset id="test.excludes">
    @ANT_EXCLUDE_TESTS@
    <exclude name="**/adm/coverity/test/junit/*"/>
    <exclude name="**/adm/installers/test/junit/*"/>
    <exclude name="**/adm/installers2/test/junit/*"/>
    <exclude name="**/adm/test/junit/*"/>
    <exclude name="**/HTMLAboutJUnitTest.*" />
    <exclude name="**/javasound/test/junit/JUnitTclTest.*" />

    <!-- FIXME: lbnl/test/junit fails under Travis-ci, but we can't easily log in to run the croom command. -->
    <exclude name="**/lbnl/test/junit/JUnitTclTest.*"/>
    <exclude name="**/KielerJUnitTest.*" />

    <!-- FIXME: HeatConductor fails to load under Travis-ci, but we can't easily log in to run the croom command. -->
    <exclude name="**/ptolemy/actor/lib/fmi/fmus/omc/test/junit/JUnitTclTest.*" />

    <!-- FIXME: EightFourInFourOutsJNI fails under Travis-ci because lib/libJNIFMU is not found. -->
    <exclude name="**/ptolemy/actor/lib/fmi/jni/test/junit/JUnitTclTest.*" />

    <!-- FIXME: Not testing fmipp right now. -->
    <exclude name="**/ptolemy/actor/lib/fmi/fmipp/test/junit/JUnitTclTest.*" />

    <!-- The R language is not typically installed. -->
    <exclude name="**/ptolemy/actor/lib/r/test/junit/JUnitTclTest.*" />

    <!-- OpenModelica is not typically installed. -->
    <exclude name="**/ptolemy/domains/openmodelica/lib/test/junit/JUnitTclTest.*" />

    <!-- PtinyOS is not likely to be installed. -->
    <exclude name="**/ptolemy/domains/ptinyos/**/JUnitTclTest.*" />

    <exclude name="**/ptolemy/cg/adapter/generic/program/procedural/c/arduino/**"/>
    <exclude name="**/ptolemy/cg/adapter/generic/program/procedural/c/mbed/**" />

    <!-- Don't try to execute these utility classes. -->
    <exclude name="**/ptolemy/util/test/junit/Auto32Tests.*" />
    <exclude name="**/ptolemy/util/test/junit/AutoTests.*" />
    <exclude name="**/ptolemy/util/test/junit/AutoCG*" />
    <exclude name="**/ptolemy/util/test/junit/AutoKnownFailedTests.*" />
    <exclude name="**/ptolemy/util/test/junit/AutoNameArchTests.*" />
    <exclude name="**/ptolemy/util/test/junit/JUnitAuto32Base.*" />
    <exclude name="**/ptolemy/util/test/junit/JUnitCG*" />
    <exclude name="**/ptolemy/util/test/junit/JUnitTclTestBase.*" />
    <exclude name="**/ptolemy/util/test/junit/JUnitTclTestRun.*" />
    <exclude name="**/ptolemy/util/test/junit/ModelKnownFailedTests.*" />
    <exclude name="**/ptolemy/util/test/junit/ModelTests.*" />
    <exclude name="**/ptolemy/util/test/junit/TclTests.*" />

    <!-- Exlude util/travis/test because it is used to indicate
         timeout problems and always fails -->
    <exclude name="**/ptolemy/util/test/travis/junit/JUnitTclTest.*" />

    <exclude name="**/lbnl/actor/lib/test/junit/JUnitTclTest.*" />

    <exclude name="**/ptserver/test/junit/FileDownloadTest.*" />
    <exclude name="**/ptserver/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptserver/test/junit/RESTGetHandlerTest.*" />
    <exclude name="**/ptserver/test/junit/RemoteModelTest.*" />
    <exclude name="**/ptserver/test/junit/ServerTest.*" />
    <exclude name="**/ptserver/test/junit/ServletTest.*" />
    <exclude name="**/ptserver/test/junit/TokenParserTest.*" />
    <exclude name="**/ptserver/test/junit/TypeParserTest.*" />
  </patternset>

  <!-- Include tests that take a long time to run.
       These tests should be excluded by test.long.excludes -->
  <patternset id="test.long.includes">
    <include name="**/JUnitCGJavaTest.java" />
    <include name="**/ptolemy/actor/lib/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/configs/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/domains/continuous/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/domains/de/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/domains/ptides/test/junit/JUnitTclTest.java" />
    <include name="**/ptolemy/plot/test/junit/JUnitTclTest.java" />
  </patternset>

  <!-- Exclude long running tests. -->
  <patternset id="test.long.excludes">
    <exclude name="**/adm/coverity/test/junit/*"/>
    <exclude name="**/adm/installers/test/junit/*"/>
    <exclude name="**/adm/installers2/test/junit/*"/>
    <exclude name="**/adm/test/junit/JUnitTclTest.*" />
    <exclude name="**/JUnitCGJavaTest.*" />
    <exclude name="**/ptolemy/actor/lib/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/configs/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/domains/continuous/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/domains/de/test/junit/JUnitTclTest*" />
    <exclude name="**/ptolemy/domains/ptides/test/junit/JUnitTclTest.*" />
    <exclude name="**/ptolemy/plot/test/junit/JUnitTclTest*" />
  </patternset>

  <!-- Include tests that take longest time to run.
       These tests should be excluded by test.longest.excludes -->
  <patternset id="test.longest.includes">
    <include name="**/ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTest.java" />
  </patternset>

  <!-- Exclude longest running tests. -->
  <patternset id="test.longest.excludes">
    <exclude name="**/ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTest.*" />
    <exclude name="**/ptolemy/vergil/basic/export/test/junit/ExportModelJUnitTestBatch*" />
  </patternset>

  <!-- JavaScript files. -->
  <patternset id="ptII.js"
	      includes="ptolemy/actor/lib/jjs/*.js,ptolemy/actor/lib/jjs/*/*.js,ptolemy/actor/lib/jjs/modules/*/*.js,org/terraswarm/accessor/accessors/web/*.js,org/terraswarm/accessor/accessors/web/*/*.js,org/terraswarm/accessor/accessors/web/*/*/*.js"
	      excludes="**/vendors/**/*.js,**/node_modules/**/*.js,**/node/**/*.js,**/node/*.js"
	      />

  <!-- Path of JavaScript tests for Mocha -->
  <fileset id="test.mocha.files" dir="${basedir}">
    <include name="**/mocha/test*.js"/>
    <exclude name="**/node_modules/**"/>
    <patternset refid="ptII.excludes" />
  </fileset>

  <!-- Ant targets. Alphabetize and add a description, please.  -->

  <!-- Targets that start with "-" are not meant to be run by users.
       These targets typically check to see if files are present.
  -->

  <target name="-check-bin-vergil" unless="bin.vergil.exists">
    <available property="bin.vergil.exists" file="bin/vergil"/>
  </target>

  <target name="-check-jsdoc">
    <available property="jsdoc.command.exists" file="${jsdoc.command}"/>
  </target>

  <target name="-check-jshint-executable" unless="jshint.executable.exists">
    <available property="jshint.executable.exists" file="${jshint.executable}"/>
  </target>
  
  <target name="-check-make-bin-vergil" unless="bin.vergil.exists">
    <condition property="bin.vergil.makeable">
      <and>
	<not>
	  <available file="bin/vergil"/>
	</not>
	<available file="mk/ptII.mk"/>
	<available file="make"
		   filepath="${env.PATH}"/>
      </and>
    </condition>
  </target>

  <!-- Check to see if node is in the path.
       Set node.in.path to true if node.exe or node are found.
       Set node.executable to node.exe or node.
  -->
  <target name="-check-node">
    <chmod perm="a+x" file="${basedir}/bin/node"/>
    <property environment="env" />
    <condition property="node.in.path">
      <or>
	<and>
          <not>
	    <os family="windows"/>
	  </not>
          <available file="node"
		     filepath="${env.PATH}"
		     property="node.executable"
		     value="node"/>
	</and>
	<and>
	  <os family="windows"/>
	  <available file="node.exe"
		     filepath="${env.PATH}"
		     property="node.executable"
		     value="node.exe"/>
	</and>
      </or>
    </condition>
    <condition property="node.executable" value="node">
      <and>
	<not>
	  <os family="windows"/>
	</not>
	<available file="node"
		   filepath="${env.PATH}"/>
      </and>
    </condition>
    <condition property="node.executable" value="node.exe">
      <and>
	  <os family="windows"/>
	  <available file="node.exe"
		     filepath="${env.PATH}"/>
      </and>
    </condition>
  </target>

  <target name="-check-node-works"
          depends="-check-node"
          if="${node.in.path}">
    <exec executable="node"
          failonerror="false"
          timeout="${timeout.npm}"
          resultproperty="return.code">

      <arg value="--version"/>
    </exec>
    <condition property="node.works">
      <not>
        <isfailure code="${return.code}"/>
      </not>
    </condition>
  </target>

  <!-- Under Windows, npm.cmd and npm are in the path, so we check
       for npm.exe and use that if it is found.
       Note that if npm.executable is set with the properties
       at the top of the file then this will not work.
  -->
  <target name="-check-npm">
    <chmod perm="a+x" file="${basedir}/bin/npm"/>
    <property environment="env" />
    <condition property="npm.executable"
	       value="npm.cmd"
	       else="npm">
      <available file="npm.cmd"
	       filepath="${env.PATH}"/>
    </condition>
  </target>

  <target name="-check-if-npmjs-org-is-up"
          depends="-check-node-works, -check-npm-works"
          if="${npm.works}"
          >
    <echo>Check to see if https://www.npmjs.org is up.</echo>
    <condition property="npmjs-org-is-up">
      <http url="https://www.npmjs.org"/>
    </condition>
  </target>

  <target name="-check-npm-works"
          depends="-check-node-works, -check-npm"
          if="${node.works}">
    <exec executable="${npm.executable}"
          failonerror="false"
          timeout="${timeout.npm}"
          resultproperty="npm.return.code">
      <arg value="--version"/>
    </exec>
    <condition property="npm.works">
       <not>
         <isfailure code="${npm.return.code}"/>
       </not>
    </condition>
  </target>

  <target name="-check-ptII-mk-file" unless="ptII.mk.exists">
    <available property="ptII.mk.exists" file="mk/ptII.mk"/>
  </target>

  <target name="-check-terraswarm-accessor-accessors-directory" unless="terraswarm-accessor-accessors-exists">
    <available property="terraswarm-accessor-accessors-exists" file="org/terraswarm/accessor/accessors"/>
  </target>

  <target name="build"
	  depends="build-message, build-subprojects, ptdoc, build-project, build-bin"
	  description="Build Ptolemy II by compiling the .java files and possibly the scripts in $PTII/bin.  This is the default ant target.  Run 'ant build-all' to build everything."/>

  <target name="build-all"
	  depends="build, build-lbnl, build-bin"
	  description="Build Ptolemy II by compiling the .java files, the scripts in $PTII/bin and the .c files in $PTII/lbnl"/>

  <target name="build-bin"
	  depends="-check-ptII-mk-file, -check-bin-vergil, -check-make-bin-vergil, build-bin-not, build-not-bin-vergil"
	  description="Run make in $PTII/bin, which creates scripts like $PTII/bin/vergil."
	  if="${bin.vergil.makeable}">
    <exec dir="${basedir}/bin"
	  executable="make"
	  timeout="${timeout.short}">
      </exec>
  </target>

  <target name="build-bin-not"
	  unless="${ptII.mk.exists}">
      <echo>
	$PTII/mk/ptII.mk does not exist, perhaps (cd $PTII; ./configure) has not been run?
      </echo>
  </target>

  <target name="build-not-bin-vergil"
	  unless="${bin.vergil.exists}">
      <echo>
	$PTII/bin/vergil does not exist, perhaps (cd $PTII/bin; make) has not been run?
	This can happen if the make binary cannot be found in the path or if $PTII/mk/ptII.mk
	is not present.
	Skipping running make in $PTII/bin, which creates scripts like $PTII/bin/vergil.
	This is only an issue if you want to invoke "$PTII/bin/vergil" instead of "ant vergil".
	However, $PTII/bin does include other scripts that may be of use.
      </echo>
  </target>

  <target name="build-message">
    <echo>Use ant -p to see other targets. JAVA_HOME=${env.JAVA_HOME}
tools.jar is necessary for compilation of doc/doclets/.
If compilation fails, try setting JAVA_HOME to the location of the JDK.
For example:
  [bldmastr@sisyphus ptII]$ which java
  /usr/bin/java
  [bldmastr@sisyphus ptII]$ ls -l /usr/bin/java
  lrwxrwxrwx 1 root root 26 Jul 31 10:12 /usr/bin/java -> /usr/java/default/bin/java
  [bldmastr@sisyphus ptII]$ export JAVA_HOME=/usr/java/default
    </echo>
  </target>

  <target name="build-lbnl"
	  description="Run make in $PTII/lbnl, which creates executables like cclient."
	  depends="-check-ptII-mk-file, build-lbnl-not"
	  if="${ptII.mk.exists}">
      <echo>
        Running make in $PTII/lbnl, which creates executables like cclient.
	Typically, this is run after build-project so that the .class files are created.
      </echo>
      <exec dir="${basedir}/lbnl"
	    executable="make"
	    timeout="${timeout.short}">
      </exec>
  </target>

  <target name="build-lbnl-not"
	  unless="${ptII.mk.exists}">
      <echo>
	$PTII/mk/ptII.mk does not exist, perhaps (cd $PTII; ./configure) has not been run?
	Skipping running make in $PTII/lbnl, which creates executables like cclient.
	This is only an issue if you are trying to build
	the Building Controls Virtual Test Bed (BCVTB).
      </echo>
  </target>

  <target name="build-subprojects"
	  depends="-build-subprojects-empty@ANT_MAVEN_SUBPROJECTS@"
	  description="Build subprojects. @ANT_MAVEN_SUBPROJECTS@"/>

  <!-- build-subprojects can't have an empty depends, so we create a small target. -->
  <target name="-build-subprojects-empty">
  </target>

  @ANT_MAVEN_BUILD_WEBSENSOR@

  <target name="build-project">
    <echo message="${ant.project.name}: ${ant.file}" />
    <javac debug="true"
	   debuglevel="${debuglevel}"
	   destdir="."
	   includeAntRuntime="false"
	   fork="true"
	   memoryinitialsize="256m"
	   memorymaximumsize="${java.maxmemory}"
           @PTMODULE_PATH_ANT@
	   source="${source}"
	   target="${target}">
      @PTADD_MODULES_ANT_JAVAC@
      <src path="${basedir}" />
      <!-- @ANT_WEBSENSOR_SRC@ -->
      <patternset refid="ptII.excludes" />
      <classpath refid="ptII.classpath" />
    </javac>
  </target>

  <target name="build-travis-opencv"
	  description="Build opencv for Travis."
	  >
      <echo>
        Building opencv for Travis.
      </echo>
      <exec dir="${basedir}"
	    executable="org/ptolemy/opencv/travis_build_opencv.sh"
	    timeout="${timeout.long}">
            <arg value="--force"/>
      </exec>
  </target>
  
  <!-- Bundle for Java 1.8 under Mac OS X.  See $PTII/doc/coding/installers.htm 
       Get vendors/misc/appbundler-1.0.jar from https://java.net/projects/appbundler/downloads
    -->
  <!--
  <taskdef name="bundleapp" 
	   classname="com.oracle.appbundler.AppBundlerTask"
	   classpath="vendors/misc/appbundler-1.0.jar"/>
   -->
  <!--
  <target name="bundle.ptiny">
    <bundleapp
       applicationCategory="public.app-category.developer-tools"
       name="Ptiny"
       copyright="Copyright (c) 1995-2019 The Regents of the University of California. All rights reserved."
       displayname="Ptiny"
       identifier="ptole"
       mainclassname="ptolemy.vergil.VergilApplication"
       outputdirectory="."
       shortversion="1.0"
       signature="PTOL"   
       >
      <runtime dir="${env.JAVA_HOME}"/>
      <option value="-classpath $APP_ROOT"/>
      <argument value="-ptiny"/>
    </bundleapp>
  </target>
  -->

  <target name="checkstyle" depends="initReports"
	  description="Run checkstyle and find common code style problems." >
    <echo>To avoid an 'OutOfMemoryError' exception, under bash, try 'export ANT_OPTS=-Xmx1024m'</echo>
    <taskdef resource="checkstyletask.properties" classpath="@CHECKSTYLE_DIR@/@CHECKSTYLE_JAR@" />
    <checkstyle config="${basedir}/ptserver/checkstyle-ptserver.xml" failOnViolation="false">
      <fileset defaultexcludes="yes"
	       dir="${basedir}">
	<patternset refid="ptII.excludes" />
      </fileset>
      <formatter type="${checkstyle.formatter}" toFile="${ptII.reports}/checkstyle_errors.${checkstyle.formatter}" />
    </checkstyle>
    <!-- <xslt in="${ptII.reports}/checkstyle_errors.xml" out="${ptII.reports}/checkstyle_report.html" style="${basedir}/ptserver/checkstyle-simple.xsl"/> -->
  </target>

  <target name="clean"
	  description="Remove all the class files.">
    <echo>
      Removing .class files.  Use 'ant cleanall' to remove files in reports/ and codeDoc/.
      Use 'ant jars.clean' to remove the jar files created by 'ant jars'.      
    </echo>
    <delete quiet="true" verbose="no" includeemptydirs="true">
      <fileset dir="${basedir}"
	       includes="**/*.class" />
      <fileset dir="${basedir}"
	       includes="*_l4j.xml" />
    </delete>
  </target>

  <target depends="clean" name="cleanall" 
	  description="Remove all the class files, the ${ptII.reports} directory, any codeDoc directories and runs clean in accessors.">
    <delete quiet="true" verbose="no" includeemptydirs="true">
      <fileset dir="${basedir}" includes="reports/**" defaultexcludes="false"/>
      <fileset dir="${basedir}" includes="**/codeDoc*/**" defaultexcludes="false"/>
    </delete>
    <subant target="clean" genericantfile="${basedir}/org/terraswarm/accessor/accessors/web/build.xml">
      <dirset dir="." includes="org/terraswarm/accessor/accessors/web"/>
    </subant>
  </target>

  <target name="doccheck"
	  depends="initReports"
	  description="Run Doccheck (http://java.sun.com/j2se/javadoc/doccheck/) to detect JavaDoc bugs." >
    <javadoc additionalparam="-notimestamp -quiet"
             destdir="${ptII.reports}/doccheck"
             docletpathref="ptII.classpath"
	     packagenames="${javadoc.packagenames}"
	     excludepackagenames="${javadoc.excludepackagenames}"
	     maxmemory="${java.maxmemory}"
	     useexternalfile="true">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="${env.JAVA_HOME}/lib/tools.jar" />
      </classpath>
      <doclet name="com.sun.tools.doclets.doccheck.DocCheck" path="vendors/sun/doccheck1.2b2/doccheck.jar" />
      <packageset dir="${basedir}" defaultexcludes="yes">
	<patternset refid="ptII.excludes" />
	<exclude name="org/ptolemy/websensor/controller/" />
	<exclude name="ptolemy/apps/websensor/src/main/java/org/ptolemy/websensor/controller/" />
	<exclude name="ptolemy/actor/ptalon/demo/ptinyos/SendAndReceiveCnt/output" />
	<exclude name="ptolemy/actor/ptalon/demo/ptinyos/SenseToLeds/output" />
	<exclude name="ptolemy/domains/ptinyos/demo/CntToLedsAndRfm/output" />
	<exclude name="ptolemy/domains/ptinyos/demo/RfmToLeds/output" />
	<exclude name="ptolemy/domains/ptinyos/demo/SendAndReceiveCnt/output" />
	<exclude name="ptolemy/domains/ptinyos/demo/SenseToLeds/output" />
      </packageset>	
    </javadoc>
  </target>

  <!-- spotbugs.  See http://spotbugs.readthedocs.io/en/latest/ant.html -->
  <target name="spotbugs"
	  depends="build, initReports"
	  description="Run Spotbugs">
    <taskdef
        resource="edu/umd/cs/findbugs/anttask/tasks.properties"
        classpath="ant/lib/spotbugs-ant.jar"/>
    <spotbugs 
       excludefilter="${basedir}/doc/spotbugs-exclude.xml"
       home="${spotbugs.home}"
       includefilter="${basedir}/doc/spotbugs-include.xml"
       jvmargs="-Xms256m -Xmx2000m"
       nested="false"
       output="xml"
       outputFile="${ptII.reports}/ptII-spotbugs.xml"
       timeout="${timeout}"
       >
       <!-- Spotbugs fails if the classpath includes *:
            File from auxiliary classpath not found: filesystem:/Users/cxh/.ptolemyII/*
            java.io.FileNotFoundException: File /Users/cxh/.ptolemyII/* doesn't exist
            At edu.umd.cs.findbugs.classfile.impl.ClassFactory.createFilesystemCodeBase(ClassFactory.java:111)
       -->
      <auxclasspath refid="ptII.classpath.optional" />
      <class location="${basedir}" />
      <sourcePath path="${basedir}" />
      @ANT_WEBSENSOR_SOURCEPATH@
    </spotbugs>
  </target>

  <target name="initReports">
    <mkdir dir="${junit.output.dir}" />
    <!-- doccheck and ojdcheck go into separate directories so that we can use
	 the Hudson HTML Publisher plugin. -->
    <mkdir dir="${ptII.reports}/doccheck" />
    <mkdir dir="${ptII.reports}/ojdcheck" />
  </target>

  <target name="installers" depends="initReports,build,jars"
	  description="Build installers by running a test, output goes to stdout." >
    <echo>
     Target to be invoked by users that create installers in the adm/gen-X.Y subdirectory.
     This target sends its output to stdout.  The test.installers target sends its output to repo
    </echo>
    <!-- printsummary takes one of One of on, off, and withOutAndErr.  Off is the default-->
    <junit fork="yes"
	   jvm="ptolemy/util/test/junit/javachdir"
	   printsummary="off"
	   showoutput="yes"
	   timeout="${timeout.longest}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <formatter type="${junit.formatter}" usefile="${usefile}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <!-- ptolemy.ptII.batchMode is checked in MessageHandler -->
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <test  name="adm.installers.test.junit.JUnitTclTest" />
    </junit>
  </target>

  <target name="jars"
	  description="Create jar files." >
    <subant antfile="jars.xml"
	    buildpath="${basedir}"
	    target="jars" />
  </target>

  <target name="jars.clean"
	  description="Remove all the jar files created by 'ant jars'." >
    <subant antfile="jars.xml"
	    buildpath="${basedir}"
	    target="jars.clean" />
  </target>

  <target name="jars.vergil"
	  description="Run vergil using jar files." >
    <subant antfile="jars.xml"
	    buildpath="${basedir}"
	    target="jars.vergil">
      <property name="ptII.classpath.optional.property" value="${ptII.classpath.optional.property}"/>
    </subant>
  </target>

  <target name="javadoc"
	  depends="javadoc.actorIndex, javadoc.base"
	  description="Generate javadoc including actor documentation and actor/demo index." /> 

  <target name="javadoc.actor"
	  depends="build, javadoc.doclets">
    <echo>
     Generate javadoc .xml files used for actor documentation.
     Generate doc/codeDoc/allNamedObjs.txt for use by javadoc.actorIndex
     For details, see $PTII/ptolemy/vergil/basic/docViewerHelp.htm
    </echo>

    <!-- java.maxmemory of 1400m might not work here, so we use
    java.maxmemory.large, which could cause problems for 32-bit
    jvms. -->

    <javadoc additionalparam="-quiet"
             destdir="doc/codeDoc"
             docletpathref="ptII.classpath"
	     packagenames="${javadoc.packagenames}"
	     excludepackagenames="${javadoc.excludepackagenames}"
	     maxmemory="${java.maxmemory.large}"
	     sourcepath="${basedir}" >
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="${env.JAVA_HOME}/lib/tools.jar" />
      </classpath>
      <doclet name="doc.doclets.PtDoclet" path="${basedir}" />
    </javadoc>
  </target>

  <target name="javadoc.actorIndex"
	  depends="javadoc.actor">
    <echo>
     Generate actor/demonstration index files.
     Read the doc/codeDoc/allNamedObjs.txt file created by javadoc.actor.
     Create the actor index.
     For details, see $PTII/ptolemy/vergil/basic/docViewerHelp.htm
    </echo>

    <!-- java.maxmemory of 1400m might not work here, so we use
    java.maxmemory.large, which could cause problems for 32-bit
    jvms. -->

    <java classname="ptolemy.moml.filter.ActorIndex"
	  fork="true"
	  maxmemory="${java.maxmemory.large}"
          @PTMODULE_PATH_ANT@
	  >
      @PTADD_MODULES_ANT@
      <arg line="doc/codeDoc/allNamedObjs.txt ptolemy/configs/doc/models.txt doc/codeDoc"/>
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="${env.JAVA_HOME}/lib/tools.jar" />
      </classpath>
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </java>
  </target>

  <target name="javadoc.base"
	  depends="javadoc.doclets">
    <echo>Generate javadoc without building the actor documentation and the actor/demonstration index</echo>
    <javadoc additionalparam="-notimestamp -quiet -tag Pt.AcceptedRating -tag Pt.ProposedRating -tag status:a:Status"
             destdir="doc/codeDoc"
	     packagenames="${javadoc.packagenames}"
	     excludepackagenames="${javadoc.excludepackagenames}"
	     maxmemory="${java.maxmemory}"
	     sourcepath="${basedir}" >
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="${env.JAVA_HOME}/lib/tools.jar" />
      </classpath>
      <taglet name="doc.doclets.RatingTaglet"
	      path="${basedir}" />
    </javadoc>
  </target>

  <target name="javadoc.doclets">
    <echo>Compile doc/doclets.
tools.jar is necessary for compilation of doc/doclets/.
If compilation fails, try setting JAVA_HOME to the location of the JDK.
For example:
  [bldmastr@sisyphus ptII]$ which java
  /usr/bin/java
  [bldmastr@sisyphus ptII]$ ls -l /usr/bin/java
  lrwxrwxrwx 1 root root 26 Jul 31 10:12 /usr/bin/java -> /usr/java/default/bin/java
  [bldmastr@sisyphus ptII]$ export JAVA_HOME=/usr/java/default
    </echo>
    <javac debug="true"
	   debuglevel="${debuglevel}"
	   destdir="."
	   includeAntRuntime="false"
	   fork="true"
	   memoryinitialsize="256m"
	   memorymaximumsize="${java.maxmemory}"
	   source="${source}"
	   target="${target}">
      <src path="${basedir}/doc/doclets" />
      <src path="${basedir}/ptolemy/util" />
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="${env.JAVA_HOME}/lib/tools.jar" />
      </classpath>
    </javac>    
  </target>

  <target name="jsdoc"
          depends="jsdoc-update, jsdoc-getFromWiki"
          description="Run jsdoc to generate documentation for JavaScript files."
          >
    <echo>Invoke jsdoc to generate documentation for .js files.
    To set this up:
       cd $PTII/vendors; git clone https://github.com/terraswarm/jsdoc.git    

    $PTII/doc/jsdoc/topREADME.md will be used as the basis of the first page.

    The output appears in doc/codeDoc/js/index.html
    </echo>
    <exec executable="${jsdoc.command}"
	  timeout="${timeout.short}">
      <arg value="--verbose" />
      <arg value="--recurse" />
      <arg value="-c" />
      <arg value="doc/jsdoc/jsdoc.json" />
      <arg value="-R" />
      <arg value="doc/jsdoc/topREADME.md" />
      <arg value="-d" />
      <arg value="doc/codeDoc/js" />

      <arg value="ptolemy/actor/lib/jjs"/>
      <arg value="org/terraswarm/accessor"/>
    </exec>
  </target>

  <target name="jsdoc-getFromWiki"
          description="Attempt to get an Accessors-specific page from the TerraSwarm wiki."
          >
    <chmod file="doc/jsdoc/makeptjsdocREADME" perm="a+x"/>
    <exec executable="doc/jsdoc/makeptjsdocREADME"
	  timeout="10000">
    </exec>
  </target>

  <target name="jsdoc-install"
	  depends="-check-jsdoc, -check-node-works, -check-npm-works, -check-if-npmjs-org-is-up, -jsdoc-no-network-message"
          if="${npmjs-org-is-up}"
	  unless="jsdoc.command.exists">
    <mkdir dir="${basedir}/node_modules"/>
    <exec executable="${npm.executable}"
          timeout="${timeout}">
      <arg value="install"/>
      <arg value="@terraswarm/jsdoc"/>
    </exec>
  </target>

  <target name="-jsdoc-no-network-message"
          unless="${npmjs-org-is-up}">
    <echo>The node binary was not found or does not work or npmjs.org was not reachable, so the network is probably down, so there is no point in trying npm.</echo>
  </target>

  <target name="jsdoc-update"
	  depends="-check-if-npmjs-org-is-up, -jsdoc-no-network-message, jsdoc-install, -check-npm-works"
          if="${npmjs-org-is-up}">
    <exec executable="${npm.executable}"
          timeout="${timeout}">
      <arg value="update"/>
      <arg value="@terraswarm/jsdoc"/>
    </exec>
  </target>

  <target name="jshint"
          depends="jshint-install"
          description="Run jshint on the .js files.  Install with 'npm install jshint'">
    <apply executable="${jshint.executable}">
      <fileset defaultexcludes="yes"
	       dir="${basedir}">
	<patternset refid="ptII.js" />
      </fileset>
    </apply>
  </target>

  <target name="jshint-checkstyle"
          description="Run jshint on the .js files and generate checkstyle output.">
    <delete file="reports/jshint.xml"/>
    <apply executable="${env.HOME}/node_modules/jshint/bin/jshint"
	   append="true"
	   output="reports/jshint.xml">
      <arg value="--reporter=checkstyle"/>
      <fileset defaultexcludes="yes"
	       dir="${basedir}">
	<patternset refid="ptII.js" />
      </fileset>
    </apply>
    <chmod file="adm/bin/fixjshint" perm="a+x"/>
    <exec executable="adm/bin/fixjshint"
	  output="reports/jshintFixed.xml"
	  timeout="${timeout.short}">
      <arg value="reports/jshint.xml"/>
    </exec>
  </target>

  <target name="jshint-install"
	  depends="-check-jshint-executable, -check-npm-works"
          description="Install jshint in ${env.HOME} using npm"
	  unless="${jshint.executable.exists}">
    <exec dir="${env.HOME}"
	  executable="${npm.executable}"
	  timeout="${timeout.short}">
      <arg value="install"/>
      <arg value="jshint"/>
    </exec>
  </target>

  <target name="junitreport"
          description="Read reports/**/TEST-*.xml files and create reports/junit/html/">

    <junitreport todir="./reports">
      <fileset dir="./reports">
        <include name="**/TEST-*.xml"/>
      </fileset>
      <report format="frames" todir="./reports/junit/html"/>
    </junitreport>
  </target>

  <!-- Ojdcheck
       We have a local copy at
       svn co svn+ssh://source.eecs.berkeley.edu/chess/ojdcheck 
       The primary difference is that ojdcheck handles enums -->
  <target name="ojdcheck"
	  depends="initReports"
	  description="Ojdcheck - An updated version of DocCheck. See http://github.com/egonw/ojdcheck">
    <javadoc additionalparam="-notimestamp -quiet -xhtml -file ${ptII.reports}/ojdcheck/ojdcheck.htm"
             docletpathref="ptII.classpath"
	     packagenames="${javadoc.packagenames}"
	     excludepackagenames="${javadoc.excludepackagenames}"
	     maxmemory="${java.maxmemory}"
	     sourcepath="${basedir}" >
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="${env.JAVA_HOME}/lib/tools.jar" />
      </classpath>
      <doclet name="com.github.ojdcheck.OpenJavaDocCheck" path="lib/ojdcheck.jar" />
    </javadoc>
  </target>

  <!-- Run ptdoc so that when a user drags in an accessor, the accessor will get a DocAttribute from
       the *PtDoc.xml file. -->
  <target name="ptdoc"
          depends="jsdoc-update, -ptdoc-no-node, -ptdoc-no-npm"
          description="Invoke jsdoc to read *.js files and generate *PtDoc.xml files suitable for Ptolemy"
          if="${npm.works}">
    <echo>
      If the ptdoc target fails, then try running:
          ant build-project build-bin
    </echo>
    <subant target="ptdoc" genericantfile="${basedir}/org/terraswarm/accessor/accessors/web/build.xml">
      <dirset dir="." includes="org/terraswarm/accessor/accessors/web"/>
    </subant>
  </target>

  <target name="-ptdoc-no-node"
          unless="${node.works}">
      <echo>
        Node was not found in the path or failed, so 'ant ptdoc' will not be run.
        This means that documentation for Accessors will not be available.
      </echo>
  </target>

  <target name="-ptdoc-no-npm"
          unless="${npm.works}">
      <echo>
        Npm was not found in the path or failed, so 'ant ptdoc' will not be run.
        This means that documentation for Accessors will not be available.
      </echo>
  </target>
  
  <target name="reports"
	  description="Generate non-test reports about the Ptolemy II code base."
	  depends="checkstyle, doccheck, spotbugs, ojdcheck" />

  <target name="resignjars"
	  description="Update the certificate of jar files in signed/**/*.jar and **/signed_*.jar.">
    <input
       message="Please enter keystore password:"
       addproperty="keypass" />
    <signjar
       alias="ptolemy" 
       keystore="${keystore}"
       lazy="false"
       storepass="${keypass}"
       >
      <path>
        <fileset dir="signed" includes="**/*.jar" />
        <fileset dir="ptolemy" includes="**/signed_*.jar" />
      </path>
    </signjar>
  </target>

  <target name="run" depends="vergil"
	  description="Invoke the Ptolemy II User Interface (alias for the vergil target)"/>

  <target name="test"
	  depends="build, test.short, test.32bit, test.long, test.longest"
	  description="Build and run the all the junit tests." />

  <target name="test.32bit"
	  description="Run the 32-bit JVM tests in auto32/..">
    <echo>
      ==test.32bit==
      This target runs 32-bit JVM tests from auto32/.

      Various test targets are available:
       test - builds and runs test.short, test.long and test.longest
       test.32bit - runs the 32-bit JVM tests in auto32/
       test.long - runs tests that are fairly long (called by the test target)
       test.longest - runs tests that are longest (called by the test target)
       test.report.all - runs batch of tests specified by a path with wildcards (output=files)
       test.report.short - runs tests that are fairly fast, generates JUnit xml
       test.short - runs tests that are fairly fast (called by the test target)
       test.single target runs single test specified by its class name (output=stdout)

      test.batch = ${test.batch}
      timeout = ${timeout.short} milliseconds

    </echo>
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir32"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.short}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <!-- Set filtertrace to off so that we see the junit stackframes -->
      <batchtest filtertrace="off">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Run all the tests in test.batch -->
	  <include name="**/junit/JUnitAuto32Test.*" />
	  <patternset refid="ptII.excludes" />
          <patternset refid="test.capecode1.excludes" />
          <patternset refid="test.capecode2.excludes" />
          <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the long running tests -->
	  <patternset refid="test.long.excludes" />
	  <!-- Exclude the longest running tests -->
	  <patternset refid="test.longest.excludes" />
	</fileset>
      </batchtest>
      <formatter type="plain" usefile="${usefile}" />
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.capecode"
          depends="test.capecode1, test.capecode2, test.capecode3"
          description="Run the CapeCode tests">
    <echo>
      ==test.capecode ==
      This target runs the Cape Code accessors tests
      timeout = ${timeout.long} milliseconds
    </echo>
  </target>

  <target name="test.capecode.echo"
          description="Echo the directories run by test.capecode and test.capecode.xml" >
    <pathconvert pathsep="${line.separator}|   |-- "
                 property="test.capecode.echo.compile">
      <fileset defaultexcludes="yes"
	       dir="${basedir}">
	<patternset refid="test.capecode1.includes" />
	<patternset refid="test.capecode2.includes" />
	<exclude name="adm/**" />
      </fileset>
    </pathconvert>
    <echo>${test.capecode.echo.compile}</echo>
  </target>


  <target name="test.capecode1" depends="initReports"
	  description="Run the first batch of Cape Code tests.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.capecode1 ==
      This target runs the first batch of Cape Code accessors tests
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <!-- Set filtertrace to off so that we see the junit stackframes -->
      <batchtest filtertrace="off">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.capecode1.includes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.capecode2" depends="initReports"
	  description="Run the second batch of Cape Code tests.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.capecode3 ==
      This target runs the second batch of Cape Code accessors tests
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <!-- Set filtertrace to off so that we see the junit stackframes -->
      <batchtest filtertrace="off">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.capecode2.includes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.capecode3" depends="initReports"
	  description="Run the third batch of Cape Code tests.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.capecode3 ==
      This target runs the third batch of Cape Code accessors tests
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <!-- Set filtertrace to off so that we see the junit stackframes -->
      <batchtest filtertrace="off">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.capecode3.includes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.capecode.xml"
          depends="test.capecode1.xml, test.capecode2.xml, test.capecode3.xml"
	  description="Run the Cape Code tests with xml output.">
  </target>

  <target name="test.capecode1.xml" depends="initReports"
	  description="Run the first batch of Cape Code tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.capecode1.xml ==
      This target runs the first batch Cape Code accessors tests and
      generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.capecode1.includes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.capecode2.xml" depends="initReports"
	  description="Run the second batch of Cape Code tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.capecode2.xml ==
      This target runs the second batch of Cape Code accessors tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.capecode2.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.capecode3.xml" depends="initReports"
	  description="Run the third batch of Cape Code tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.capecode3.xml ==
      This target runs the thirdbatch of Cape Code accessors tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.capecode3.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>


  <target name="test.core1.xml" depends="initReports"
	  description="Run the first batch of core tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.core1.xml ==
      This target runs the first batch of core tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.core1.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.core2.excludes" />
	  <patternset refid="test.core3.excludes" />
	  <patternset refid="test.core4.excludes" />
	  <patternset refid="test.core5.excludes" />
	  <patternset refid="test.core6.excludes" />
	  <patternset refid="test.core7.excludes" />
	  <patternset refid="test.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.core2.xml" depends="initReports"
	  description="Run the second batch of core tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.core2.xml ==
      This target runs the second batch of core tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.core2.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.core1.excludes" />
	  <patternset refid="test.core3.excludes" />
	  <patternset refid="test.core4.excludes" />
	  <patternset refid="test.core5.excludes" />
	  <patternset refid="test.core6.excludes" />
	  <patternset refid="test.core7.excludes" />
	  <patternset refid="test.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>
  
  <target name="test.core3.xml" depends="initReports"
	  description="Run the third batch of core tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.core3.xml ==
      This target runs the third batch of core tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.core3.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.core1.excludes" />
	  <patternset refid="test.core2.excludes" />
	  <patternset refid="test.core4.excludes" />
	  <patternset refid="test.core5.excludes" />
	  <patternset refid="test.core6.excludes" />
	  <patternset refid="test.core7.excludes" />
	  <patternset refid="test.excludes" />
          <!-- Don't exclude adm/** here because we want to run adm/test. -->
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.core4.xml" depends="initReports"
	  description="Run the fourth batch of core tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.core4.xml ==
      This target runs the fourth batch of core tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.core4.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.core1.excludes" />
	  <patternset refid="test.core2.excludes" />
	  <patternset refid="test.core3.excludes" />
	  <patternset refid="test.core5.excludes" />
	  <patternset refid="test.core6.excludes" />
	  <patternset refid="test.core7.excludes" />
	  <patternset refid="test.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.core5.xml" depends="initReports"
	  description="Run the fifth batch of core tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.core5.xml ==
      This target runs the fifth batch of core tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.core5.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.core1.excludes" />
	  <patternset refid="test.core2.excludes" />
	  <patternset refid="test.core3.excludes" />
	  <patternset refid="test.core4.excludes" />
	  <patternset refid="test.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.core6.xml" depends="initReports"
	  description="Run the sixth batch of core tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.core6.xml ==
      This target runs the sixth batch of core tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.core6.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.core1.excludes" />
	  <patternset refid="test.core2.excludes" />
	  <patternset refid="test.core3.excludes" />
	  <patternset refid="test.core4.excludes" />
	  <patternset refid="test.core5.excludes" />
	  <patternset refid="test.core7.excludes" />
	  <patternset refid="test.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.core7.xml" depends="initReports"
	  description="Run the seventh batch of core tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.core7.xml ==
      This target runs the seventh batch of core tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.core7.includes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.core1.excludes" />
	  <patternset refid="test.core2.excludes" />
	  <patternset refid="test.core3.excludes" />
	  <patternset refid="test.core4.excludes" />
	  <patternset refid="test.core5.excludes" />
	  <patternset refid="test.core6.excludes" />
	  <patternset refid="test.excludes" />
	  <exclude name="adm/**" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.export1.xml" depends="initReports"
	  description="Run the first batch of export demo tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.export1.xml ==
      This target runs the first batch of export demo tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.export1.includes" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.export2.xml" depends="initReports"
	  description="Run the second batch of export demo tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.export2.xml ==
      This target runs the second batch of export demo tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.export2.includes" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>
  
  <target name="test.export3.xml" depends="initReports"
	  description="Run the third batch of export demo tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.export3.xml ==
      This target runs the third batch of export demo tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.export3.includes" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.export4.xml" depends="initReports"
	  description="Run the fourth batch of export demo tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.export4.xml ==
      This target runs the fourth batch of export demo tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.export4.includes" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.export5.xml" depends="initReports"
	  description="Run the fifth batch of export demo tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.export5.xml ==
      This target runs the fifth batch of export demo tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.export5.includes" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.export6.xml" depends="initReports"
	  description="Run the sixth batch of export demo tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.export6.xml ==
      This target runs the sixth batch of export demo tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.export6.includes" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.export7.xml" depends="initReports"
	  description="Run the seventh batch of export demo tests with xml output.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.export7.xml ==
      This target runs the seventh batch of export demo tests
      and generates JUnit xml output.
      timeout = ${timeout.long} milliseconds
    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <patternset refid="test.export7.includes" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <!-- Cobertura: code coverage tool -->
  <target name="test.cobertura"
	  depends="build, initReports"
	  description="Run code coverage on all the tests"
	  >
    <echo> 
      To run code coverage on just one test, use 
      ant test.cobertura.single
    </echo>
    <exec executable="date"/>
    <property name="instrumented.dir" value="${ptII.reports}/instrumented" />
    <property name="coveragereport.dir" value="${ptII.reports}/coveragereport" />

    <taskdef classpathref="cobertura.classpath" resource="tasks.properties" />

    <delete file="cobertura.ser" />
    <delete dir="${instrumented.dir}" />
    
    <cobertura-instrument todir="${instrumented.dir}">
      <fileset defaultexcludes="yes"
	       dir="${basedir}">
	<include name="org/ptolemy/**/*.class" />
	<include name="ptolemy/**/*.class" />
	<exclude name="**/test/*.class" />
	<exclude name="**/junit/*.class" />
      </fileset>
    </cobertura-instrument>


    <!-- About forkmode,
	 http://cobertura.sourceforge.net/anttaskreference.html says:

	 "For this same reason, if you're using ant 1.6.2 or higher
	 then you might want to set forkmode="once" This will cause
	 only one JVM to be started for all your JUnit tests, and will
	 reduce the overhead of Cobertura reading/writing the coverage
	 data file each time a JVM starts/stops."

	 http://ant.apache.org/manual/Tasks/junit.html says:
	 "Controls how many Java Virtual Machines get created if you want to
	 fork some tests. Possible values are "perTest" (the default),
	 "perBatch" and "once". "once" creates only a single Java VM for all
	 tests while "perTest" creates a new VM for each TestCase
	 class. "perBatch" creates a VM for each nested <batchtest> and one
	 collecting all nested <test>s. Note that only tests with the same
	 settings of filtertrace, haltonerror, haltonfailure, errorproperty and
	 failureproperty can share a VM, so even if you set forkmode to "once",
	 Ant may have to create more than a single Java VM. This attribute is
	 ignored for tests that don't get forked into a new Java VM. since Ant
	 1.6.2"

	 However, we have to use "perTest" because javachdir needs to be
	 run for each test so that we run the test from the test directory.
      -->

    <echo> test.cobertura: Run the short tests with timeout=${timeout.short}</echo>
    <exec executable="date"/>
    <junit fork="yes"
	   forkmode="perTest"
	   jvm="ptolemy/util/test/junit/javachdir"
	   maxmemory="${java.maxmemory}"
	   printsummary="withOutAndErr"
	   showoutput="no"
	   timeout="${timeout.short}">
      <classpath>
	<path location="${instrumented.dir}" />
	<path refid="ptII.classpath" />
	<path refid="cobertura.classpath" />
      </classpath>
      <formatter type="${junit.formatter}" />
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Run all the tests in test.batch -->
	  <include name="${test.batch}" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the long running tests -->
	  <patternset refid="test.long.excludes" />
	  <!-- Exclude the longest running tests -->
	  <patternset refid="test.longest.excludes" />
	</fileset>
      </batchtest>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <sysproperty key="net.sourceforge.cobertura.datafile" file="${basedir}/cobertura.ser" />
    </junit>

    <echo> test.cobertura: Run the 32-bit tests with timeout=${timeout.short}</echo>
    <exec executable="date"/>
    <junit fork="yes"
	   forkmode="perTest"
	   jvm="ptolemy/util/test/junit/javachdir32"
	   maxmemory="${java.maxmemory}"
	   printsummary="withOutAndErr"
	   showoutput="no"
	   timeout="${timeout.short}">
      <classpath>
	<path location="${instrumented.dir}" />
	<path refid="ptII.classpath" />
	<path refid="cobertura.classpath" />
      </classpath>
      <formatter type="${junit.formatter}" />
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Run the 32-bit tests-->
	  <include name="**/junit/JUnitAuto32Test.*" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the long running tests -->
	  <patternset refid="test.long.excludes" />
	  <!-- Exclude the longest running tests -->
	  <patternset refid="test.longest.excludes" />
	</fileset>
      </batchtest>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <sysproperty key="net.sourceforge.cobertura.datafile" file="${basedir}/cobertura.ser" />
    </junit>

    <echo> test.cobertura: Run the long tests with timeout=${timeout.long}</echo>
    <exec executable="date"/>
    <junit fork="yes"
	   forkmode="perTest"
	   jvm="ptolemy/util/test/junit/javachdir"
	   maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="no"
	   timeout="${timeout.long}">
      <classpath>
	<path location="${instrumented.dir}" />
	<path refid="ptII.classpath" />
	<path refid="cobertura.classpath" />
      </classpath>
      <formatter type="${junit.formatter}" />
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Include the long running tests. -->
	  <patternset refid="test.long.includes" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
	  <!-- Exclude the longest running tests -->
	  <patternset refid="test.longest.excludes" />
	</fileset>
      </batchtest>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <jvmarg value="-XX:+HeapDumpOnOutOfMemoryError"/>
      <jvmarg value="-XX:HeapDumpPath=${env.HOME}/ptII.test.longest.hprof"/>
      <sysproperty key="net.sourceforge.cobertura.datafile" file="${basedir}/cobertura.ser" />
    </junit>

    <echo> test.cobertura: Run the longest tests with timeout=${timeout.longest}</echo>
    <exec executable="date"/>
    <junit fork="yes"
	   forkmode="perTest"
	   jvm="ptolemy/util/test/junit/javachdir"
	   maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.longest}">
      <classpath>
	<path location="${instrumented.dir}" />
	<path refid="ptII.classpath" />
	<path refid="cobertura.classpath" />
      </classpath>
      <formatter type="${junit.formatter}" />
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Include the longest running tests. -->
	  <patternset refid="test.longest.includes" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the long running tests -->
	  <patternset refid="test.long.excludes" />
	</fileset>
      </batchtest>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <jvmarg value="-Dptolemy.ptII.exportHTML.linkToJNLP=true"/>
      <jvmarg value="-Dptolemy.ptII.exportHTML.usePtWebsite=true"/>
      <jvmarg value="-XX:MaxPermSize=256m"/>
      <jvmarg value="-XX:+HeapDumpOnOutOfMemoryError"/>
      <jvmarg value="-XX:HeapDumpPath=${env.HOME}/ptII.test.longest.hprof"/>
      <sysproperty key="net.sourceforge.cobertura.datafile" file="${basedir}/cobertura.ser" />
    </junit>

    <cobertura-report format="xml" destdir="${coveragereport.dir}" srcdir="${basedir}" />
  </target>


  <!-- Cobertura: code coverage tool.  Run the longest tests. -->
  <target name="test.cobertura.longest" depends="build, initReports">
    <echo>Run the longest tests using code coverage
    </echo>
    <exec executable="date"/>
    <property name="instrumented.dir" value="${ptII.reports}/instrumented" />
    <property name="coveragereport.dir" value="${ptII.reports}/coveragereport" />

    <taskdef classpathref="cobertura.classpath" resource="tasks.properties" />

    <delete file="cobertura.ser" />
    <delete dir="${instrumented.dir}" />
    
    <cobertura-instrument todir="${instrumented.dir}">
      <fileset defaultexcludes="yes"
	       dir="${basedir}">
	<include name="org/ptolemy/**/*.class" />
	<include name="ptolemy/**/*.class" />
	<exclude name="**/test/*.class" />
	<exclude name="**/junit/*.class" />
      </fileset>
    </cobertura-instrument>


    <!-- About forkmode,
	 http://cobertura.sourceforge.net/anttaskreference.html says:

	 "For this same reason, if you're using ant 1.6.2 or higher
	 then you might want to set forkmode="once" This will cause
	 only one JVM to be started for all your JUnit tests, and will
	 reduce the overhead of Cobertura reading/writing the coverage
	 data file each time a JVM starts/stops."

	 http://ant.apache.org/manual/Tasks/junit.html says:
	 "Controls how many Java Virtual Machines get created if you want to
	 fork some tests. Possible values are "perTest" (the default),
	 "perBatch" and "once". "once" creates only a single Java VM for all
	 tests while "perTest" creates a new VM for each TestCase
	 class. "perBatch" creates a VM for each nested <batchtest> and one
	 collecting all nested <test>s. Note that only tests with the same
	 settings of filtertrace, haltonerror, haltonfailure, errorproperty and
	 failureproperty can share a VM, so even if you set forkmode to "once",
	 Ant may have to create more than a single Java VM. This attribute is
	 ignored for tests that don't get forked into a new Java VM. since Ant
	 1.6.2"

	 However, we have to use "perTest" because javachdir needs to be
	 run for each test so that we run the test from the test directory.
      -->

    <echo> Run the longest tests with timeout=${timeout.longest} </echo>
    <junit fork="yes"
	   forkmode="perTest"
	   jvm="ptolemy/util/test/junit/javachdir"
	   maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.longest}">
      <classpath>
	<path location="${instrumented.dir}" />
	<path refid="ptII.classpath" />
	<path refid="cobertura.classpath" />
      </classpath>
      <formatter type="${junit.formatter}" />
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Include the longest running tests. -->
	  <patternset refid="test.longest.includes" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the long running tests -->
	  <patternset refid="test.long.excludes" />
	</fileset>
      </batchtest>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <jvmarg value="-Dptolemy.ptII.exportHTML.linkToJNLP=true"/>
      <jvmarg value="-Dptolemy.ptII.exportHTML.usePtWebsite=true"/>
      <jvmarg value="-XX:MaxPermSize=256m"/>
      <sysproperty key="net.sourceforge.cobertura.datafile" file="${basedir}/cobertura.ser" />
    </junit>

    <cobertura-report format="xml" destdir="${coveragereport.dir}" srcdir="${basedir}" />
  </target>


  <!-- Cobertura: code coverage tool.  Run a single test. -->
  <target name="test.cobertura.single"
	  depends="build, initReports"
	  description="Run code coverage on a single test.">
    <echo>
      ==test.corbertura.single==
      To run code coverage on a different test, use the -Dtest.name, for example:
        ant test.cobertura.single -Dtest.name=ptolemy.configs.test.junit.JUnitTclTest

      The formatter is controlled by junit.single.formatter, which defaults to plain
      Other values are brief, failure and xml.
    </echo>
    <exec executable="date"/>
    <property name="instrumented.dir" value="${ptII.reports}/instrumented" />
    <property name="coveragereport.dir" value="${ptII.reports}/coveragereport" />

    <taskdef classpathref="cobertura.classpath" resource="tasks.properties" />

    <delete file="cobertura.ser" />
    <delete dir="${instrumented.dir}" />
    
    <cobertura-instrument todir="${instrumented.dir}">
      <fileset defaultexcludes="yes"
	       dir="${basedir}">
	<include name="org/ptolemy/**/*.class" />
	<include name="ptolemy/**/*.class" />
	<exclude name="**/test/*.class" />
	<exclude name="**/junit/*.class" />
      </fileset>
    </cobertura-instrument>


    <!-- About forkmode,
	 http://cobertura.sourceforge.net/anttaskreference.html says:

	 "For this same reason, if you're using ant 1.6.2 or higher
	 then you might want to set forkmode="once" This will cause
	 only one JVM to be started for all your JUnit tests, and will
	 reduce the overhead of Cobertura reading/writing the coverage
	 data file each time a JVM starts/stops."

	 http://ant.apache.org/manual/Tasks/junit.html says:
	 "Controls how many Java Virtual Machines get created if you want to
	 fork some tests. Possible values are "perTest" (the default),
	 "perBatch" and "once". "once" creates only a single Java VM for all
	 tests while "perTest" creates a new VM for each TestCase
	 class. "perBatch" creates a VM for each nested <batchtest> and one
	 collecting all nested <test>s. Note that only tests with the same
	 settings of filtertrace, haltonerror, haltonfailure, errorproperty and
	 failureproperty can share a VM, so even if you set forkmode to "once",
	 Ant may have to create more than a single Java VM. This attribute is
	 ignored for tests that don't get forked into a new Java VM. since Ant
	 1.6.2"

	 However, we have to use "perTest" because javachdir needs to be
	 run for each test so that we run the test from the test directory.
      -->

    <echo> Run the tests in test.name=${test.name} with timeout=${timeout.short} </echo>
    <junit fork="yes"
	   forkmode="perTest"
	   jvm="ptolemy/util/test/junit/javachdir"
	   maxmemory="${java.maxmemory}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.short}">
      <classpath>
	<path location="${instrumented.dir}" />
	<path refid="ptII.classpath" />
	<path refid="cobertura.classpath" />
      </classpath>
      <formatter type="${junit.single.formatter}" />
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <test  name="${test.name}" />
      <sysproperty key="net.sourceforge.cobertura.datafile" file="${basedir}/cobertura.ser" />
    </junit>

    <cobertura-report format="xml" destdir="${coveragereport.dir}" srcdir="${basedir}" />
  </target>


  <target name="test.installers" depends="initReports,build,jars"
	  description="Build installers by running a test, output goes to reports/junit." >
    <echo>
     Target to be invoked by the nightly build that create installers in the adm/gen-X.Y subdirectory.
     This test generates .xml files in reports/junit.  The installers target sends its output to stdout.
    </echo>
    <!-- printsummary takes one of One of on, off, and withOutAndErr.  Off is the default-->
    <junit fork="yes"
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.longest}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <formatter type="${junit.formatter}" usefile="true"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <test  name="adm.installers.test.junit.JUnitTclTest"
	     todir="${junit.output.dir}"
	     />
    </junit>
  </target>

  <target name="test.long"
	  description="Run the long-running junit tests in a directory or directories.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.long==
      This target runs test specified by a file name and generates
      human readable output to stdout.

      The default is to run all the fairly long-running tests.

      Various test targets are available:
       test - builds and runs test.short, test.long and test.longest
       test.32bit - runs the 32-bit JVM tests in auto32/
       test.long - runs tests that are fairly long (called by the test target)
       test.longest - runs tests that are longest (called by the test target)
       test.report.all - runs batch of tests specified by a path with wildcards (output=files)
       test.report.short - runs tests that are fairly fast, generates JUnit xml
       test.short - runs tests that are fairly fast (~30 min., called by the test target)
       test.single target runs single test specified by its class name (output=stdout)

      timeout = ${timeout.long} milliseconds

    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <!-- Set filtertrace to off so that we see the junit stackframes -->
      <batchtest filtertrace="off">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Include the long running tests. -->
	  <patternset refid="test.long.includes" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the longest running tests -->
	  <patternset refid="test.longest.excludes" />
	</fileset>
      </batchtest>
      <formatter type="plain" usefile="${usefile}" />
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>


  <target name="test.longest"
	  description="Run the longest-running junit tests in a directory or directories.">
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <echo>
      ==test.longest==
      This target runs test specified by a file name and generates
      human readable output to stdout.

      Various test targets are available:
       test - builds and runs test.short, test.long and test.longest
       test.32bit - runs the 32-bit JVM tests in auto32/
       test.long - runs tests that are fairly long (called by the test target)
       test.longest - runs tests that are longest (called by the test target)
       test.report.all - runs batch of tests specified by a path with wildcards (output=files)
       test.report.short - runs tests that are fairly fast, generates JUnit xml
       test.short - runs tests that are fairly fast (~30 min., called by the test target)
       test.single target runs single test specified by its class name (output=stdout)

      timeout = ${timeout.longest} milliseconds

    </echo>
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.longest}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <!-- Set filtertrace to off so that we see the junit stackframes -->
      <batchtest filtertrace="off">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Include the longest running tests. -->
	  <patternset refid="test.longest.includes" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
          <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the long running tests -->
	  <patternset refid="test.long.excludes" />
	</fileset>
      </batchtest>
      <formatter type="plain" usefile="${usefile}" />
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <jvmarg value="-Dptolemy.ptII.exportHTML.linkToJNLP=true"/>
      <jvmarg value="-Dptolemy.ptII.exportHTML.usePtWebsite=true"/>
      <jvmarg value="-XX:MaxPermSize=256m"/>
      <jvmarg value="-XX:+HeapDumpOnOutOfMemoryError"/>
      <jvmarg value="-XX:HeapDumpPath=${env.HOME}/ptII.test.longest.hprof"/>
    </junit>
  </target>

  <target name="test.mocha" depends="initReports"
          description="Use mocha to test node.js and generate output on stdout.">
    <echo>
      ==test.mocha==
      This target uses mocha to test Node.js tests in **/mocha/test*.js files.
      This target requires setup:
        sudo npm install -g mocha
        sudo npm install -g mocha-junit-reporter
      See https://chess.eecs.berkeley.edu/ptexternal/wiki/Main/JSMocha
    </echo>

    <pathconvert refid="test.mocha.files"
		 pathsep=" "
		 property="converted"/>

    <echo>Running test.mocha in ${converted}</echo>
    <exec executable="mocha"
	  timeout="${timeout.short}">
      <arg line="${converted}"/>
    </exec>
  </target>
  
  <target name="test.mocha.xml" depends="initReports"
          description="Use mocha to test node.js and generate JUnit compatible xml output.">
    <echo>
      ==test.mocha.xml==
      This target uses mocha to test Node.js tests in **/mocha/test*.js files.
      This target requires setup:
        sudo npm install -g mocha
        sudo npm install -g mocha-junit-reporter
      See https://chess.eecs.berkeley.edu/ptexternal/wiki/Main/JSMocha
      The output will appear in reports/junit/mochaJUnit.xml
    </echo>
    <pathconvert refid="test.mocha.files"
		 pathsep=" "
		 property="converted"/>

    <exec executable="mocha"
	  timeout="${timeout.short}">
      <arg value="--reporter"/>
      <arg value="mocha-junit-reporter"/>
      <arg value="--reporter-options"/>
      <arg value="mochaFile=reports/junit/mochaJUnit.xml"/>
      <arg line="${converted}"/>
    </exec>
  </target>

  <target name="test.report" depends="test.report.all"/>

  <target name="test.report.all" depends="build, initReports, installers"
	  description="Run the all the junit tests by path (**/junit/*.java), results in $PTII/reports.">
    <echo>
      ==test.report==
      This target runs all the tests (short, long, longest) and generates
      ${junit.formatter} output in ${junit.output.dir}.
 
       test - builds and runs test.short, test.long and test.longest
       test.32bit - runs the 32-bit JVM tests in auto32/
       test.long - runs tests that are fairly long (called by the test target)
       test.longest - runs tests that are longest (called by the test target)
       test.report.all - runs batch of tests specified by a path with wildcards (output=files)
       test.report.short - runs tests that are fairly fast, generates JUnit xml
       test.short - runs tests that are fairly fast (~30 min., called by the test target)
       test.single target runs single test specified by its class name (output=stdout)

      test.batch = ${test.batch}
      timeout.long = ${timeout.long}
    </echo>
    <junit fork="yes"
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.long}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <batchtest todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <include name="${test.batch}" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
          <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}" />
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.report.short" depends="build, initReports"
          description="Run the short-running junit tests in a directory or directories, results in $PTII/reports.">
   <echo>
     ==test.report.short==
     This target runs test specified by a file name and generates 
     ${junit.formatter} output in ${junit.output.dir}.

     The default is to run all the fairly short-running tests.
     On a fast machine, these tests take around 30 minutes.

     To run different tests, use -Dtest.batch, for example:
        ant test.short -Dtest.batch=ptolemy/domains/continuous/**/junit/*.java

      test - builds and runs test.short, test.long and test.longest
      test.32bit - runs the 32-bit JVM tests in auto32/
      test.long - runs tests that are fairly long (called by the test target)
      test.longest - runs tests that are longest (called by the test target)
      test.report.all - runs batch of tests specified by a path with wildcards (output=files)
      test.report.short - runs tests that are fairly fast, generates JUnit xml
      test.short - runs tests that are fairly fast (~30 min., called by the test target)

      test.single target runs single test specified by its class name (output=stdout)

      test.batch = ${test.batch}
      timeout.long = ${timeout.long}
   </echo>
   <junit fork="yes"
          jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
          printsummary="withOutAndErr"
          showoutput="yes"
          timeout="${timeout.long}">
     <classpath>
       <path refid="ptII.classpath" />
       <pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
     </classpath>
     <batchtest todir="${junit.output.dir}">
       <fileset defaultexcludes="yes"
	 dir="${basedir}">
         <include name="${test.batch}" />
         <patternset refid="ptII.excludes" />
         <patternset refid="test.32bit.excludes" />
         <patternset refid="test.capecode1.excludes" />
	 <patternset refid="test.capecode2.excludes" />
         <patternset refid="test.capecode3.excludes" />
         <patternset refid="test.excludes" />
         <!-- Exclude the long running tests -->
         <patternset refid="test.long.excludes" />
         <!-- Exclude the longest running tests -->
         <patternset refid="test.longest.excludes" />
       </fileset>
     </batchtest>
     <formatter type="${junit.formatter}" />
     <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
     <jvmarg value="-Dptolemy.ptII.batchMode=true" />
   </junit>
  </target>

  <target name="test.short"
	  description="Run the short-running junit tests in a directory or directories.">
    <echo>
      ==test.short==
      This target runs test specified by a file name and generates 
      human readable output to stdout.

      The default is to run all the fairly short-running tests.
      On a fast machine, these tests take around 30 minutes.

      To run different tests, use -Dtest.batch, for example:
         ant test.short -Dtest.batch=ptolemy/domains/continuous/**/junit/*.java
      
      Various test targets are available:
       test - builds and runs test.short, test.long and test.longest
       test.32bit - runs the 32-bit JVM tests in auto32/
       test.long - runs tests that are fairly long (called by the test target)
       test.longest - runs tests that are longest (called by the test target)
       test.report.all - runs batch of tests specified by a path with wildcards (output=files)
       test.report.short - runs tests that are fairly fast, generates JUnit xml
       test.short - runs tests that are fairly fast (called by the test target)
       test.single target runs single test specified by its class name (output=stdout)

      test.batch = ${test.batch}
      timeout = ${timeout.short} milliseconds

    </echo>
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.short}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <!-- Set filtertrace to off so that we see the junit stackframes -->
      <batchtest filtertrace="off">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <!-- Run all the tests in test.batch -->
	  <include name="${test.batch}" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the long running tests -->
	  <patternset refid="test.long.excludes" />
	  <!-- Exclude the longest running tests -->
	  <patternset refid="test.longest.excludes" />
	</fileset>
      </batchtest>
      <formatter type="plain" usefile="${usefile}" />
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="test.short.echo"
          description="Echo the directories run by test.short and test.report.short" >
    <pathconvert pathsep="${line.separator}|   |-- "
                 property="test.short.echo.compile">
      <fileset defaultexcludes="yes"
	       dir="${basedir}">
	  <!-- Run all the tests in test.batch -->
	  <include name="${test.batch}" />
	  <patternset refid="ptII.excludes" />
	  <patternset refid="test.32bit.excludes" />
	  <patternset refid="test.capecode1.excludes" />
	  <patternset refid="test.capecode2.excludes" />
	  <patternset refid="test.capecode3.excludes" />
	  <patternset refid="test.excludes" />
	  <!-- Exclude the long running tests -->
	  <patternset refid="test.long.excludes" />
	  <!-- Exclude the longest running tests -->
	  <patternset refid="test.longest.excludes" />
	</fileset>
    </pathconvert>
    <echo>${test.short.echo.compile}</echo>
  </target>

  <target name="test.single" depends="initReports"
	  description="Run a single JUnit test by classname (ptolemy.actor.lib.test.junit.JUnitTclTest).">
    <echo>
      ==test.single==
      Run a single JUnit test by classname (ptolemy.actor.lib.test.junit.JUnitTclTest).

      To run a different test, use
         -Dtest.name=ptolemy.actor.lib.test.junit.JUnitTclTest
      and 
         -Djunit.single.formatter=xml|brief|plain|failure (default is plain)

      For example:
      * Run all the tests in actor/lib/test
          ant test.single
      * Run all the tests in kernel/test
          ant test.single -Dtest.name=ptolemy.kernel.test.junit.JUnitTclTest

      Various other test targets are available:
       test - builds and runs test.short, test.long and test.longest
       test.32bit - runs the 32-bit JVM tests in auto32/
       test.cobertura - run tests using code coverage
       test.long - runs tests that are fairly long (called by the test target)
       test.longest - runs tests that are longest (called by the test target)
       test.report.all - runs batch of tests specified by a path with wildcards (output=files)
       test.report.short - runs tests that are fairly fast, generates JUnit xml
       test.short - runs tests that are fairly fast (called by the test target)
       test.single target runs single test specified by its class name (output=stdout)

    </echo>
    <!-- printsummary takes one of One of on, off, and withOutAndErr.  Off is the default-->
    <junit fork="yes"
	   jvm="ptolemy/util/test/junit/javachdir"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.short}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <formatter type="${junit.single.formatter}" usefile="${usefile}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
      <test todir="${junit.output.dir}"
	    name="${test.name}" />
    </junit>
  </target>

  <target name="test.travis.timeout.fail.xml"
	  description="Run a test that will fail, indicating a timeout problem with Travis.">
    <echo>
      ==test.travis.timeout.fail.xml==
      This target runs a test that will fail, indicating a timeout problem with Travis.

      If the Travis continuous integration system needs to update the OpenCV cache,
      then there is a chance that building OpenCV will take up too much time and the
      rest of the build will fail.

      If too much time is taken up, then we should skip running the tests, but
      then we need to have a test failure that indicates that the other tests did not run.

    </echo>
    <!-- fork="yes" is need so that javchdir can cd to the test/ directory -->
    <!-- printsummary = on, off, and withOutAndErr. withOutAndErr prints stdout and stderr -->
    <junit fork="yes" 
	   jvm="ptolemy/util/test/junit/javachdir"
           maxmemory="${java.maxmemory.large}"
	   printsummary="withOutAndErr"
	   showoutput="yes"
	   timeout="${timeout.short}">
      <classpath>
	<path refid="ptII.classpath" />
	<pathelement location="lib/lib/JUnitParams-0.3.0.jar"/>
      </classpath>
      <!-- Set filtertrace to off so that we see the junit stackframes -->
      <batchtest filtertrace="off"
                 todir="${junit.output.dir}">
	<fileset defaultexcludes="yes"
		 dir="${basedir}">
	  <include name="**/ptolemy/util/test/travis/junit/JUnitTclTest.java"/>
	</fileset>
      </batchtest>
      <formatter type="${junit.formatter}"/>
      <jvmarg value="-Dptolemy.ptII.dir=${basedir}" />
      <jvmarg value="-Dptolemy.ptII.batchMode=true" />
    </junit>
  </target>

  <target name="update"
	  description="Update the tree from the SVN repository."
          depends="update-accessors">
    <exec executable="svn"
	  timeout="${timeout.short}">
      <arg value="update"/>
    </exec>
  </target>

  <target name="update-accessors"
	  description="Update the org/terraswarm/accessor/accessors from the SVN repository."
          depends="-check-terraswarm-accessor-accessors-directory"
          if="${terraswarm-accessor-accessors-exists}">
    <exec dir="org/terraswarm/accessor/accessors"
          executable="svn"
	  timeout="${timeout.short}">
      <arg value="update"/>
    </exec>
  </target>

  <target name="vergil" depends="build-project"
	  description="Invoke the Ptolemy II User Interface.  To run a model, use ant vergil -Dmodel=model.xml"
	  >
    <java classname="ptolemy.vergil.VergilApplication"
	  fork="true"
          @PTMODULE_PATH_ANT@
          >
      @PTADD_MODULES_ANT@
      <classpath>
	<pathelement location="/Users/cxh/ptII" />
	<path refid="ptII.classpath"/>
      </classpath>
      <jvmarg value="-Xmx${java.maxmemory}"/>
      <arg line="${model}"/>
    </java>
  </target>

</project>
